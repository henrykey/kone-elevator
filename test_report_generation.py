#!/usr/bin/env python
# Author: IBC-AI CO.
"""
ç‹¬ç«‹çš„æŠ¥å‘Šç”Ÿæˆæµ‹è¯•è„šæœ¬
æµ‹è¯•KONEéªŒè¯ç³»ç»Ÿçš„æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼Œä¸ä¾èµ–API    for test_id, name, category, duration in skip_tests:
        test_results.append(TestResult(
            test_id=test_id,
            name=name,
            description=f"Test case for {name.lower()}",
            expected_result="Test execution should be successful and return expected results",
            test_result="SKIP",
            status=TestStatus.SKIP,
            duration_ms=duration,
            category=category.value
        ))import asyncio
import logging
from datetime import datetime
from pathlib import Path
from report_generator import ReportGenerator, TestResult
from test_case_mapper import TestCaseMapper, TestCategory
from building_data_manager import BuildingDataManager

# å®šä¹‰æµ‹è¯•çŠ¶æ€å¸¸é‡
class TestStatus:
    PASS = "PASS"
    FAIL = "FAIL"
    SKIP = "SKIP"
    ERROR = "ERROR"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_mock_test_results() -> list:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æµ‹è¯•ç»“æœæ•°æ®"""
    test_results = []
    
    # æˆåŠŸçš„æµ‹è¯•ç”¨ä¾‹
    success_tests = [
        ("TC001", "APIåˆå§‹åŒ–æµ‹è¯•", TestCategory.INITIALIZATION, 125.5),
        ("TC002", "WebSocketè¿æ¥æµ‹è¯•", TestCategory.INITIALIZATION, 89.2),
        ("TC003", "åŸºç¡€å‘¼å«æµ‹è¯•", TestCategory.CALL_MANAGEMENT, 234.1),
        ("TC004", "æ¥¼å±‚çŠ¶æ€æŸ¥è¯¢", TestCategory.STATUS_MONITORING, 45.8),
        ("TC005", "ç”µæ¢¯çŠ¶æ€ç›‘æ§", TestCategory.STATUS_MONITORING, 67.3),
        ("TC006", "æ€§èƒ½åŸºå‡†æµ‹è¯•", TestCategory.PERFORMANCE, 1250.0),
        ("TC007", "å¹¶å‘å‘¼å«æµ‹è¯•", TestCategory.PERFORMANCE, 2100.5),
    ]
    
    for test_id, name, category, duration in success_tests:
        test_results.append(TestResult(
            test_id=test_id,
            name=name,
            description=f"Test case for {name.lower()}",
            expected_result="Test execution should be successful and return expected results",
            test_result="PASS",
            status=TestStatus.PASS,
            duration_ms=duration,
            category=category.value,
            response_data={"response_time": duration, "status_code": 200}
        ))
    
    # å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
    failed_tests = [
        ("TC008", "é”™è¯¯ä»£ç å¤„ç†", TestCategory.ERROR_HANDLING, 78.9, "APIè¿”å›é”™è¯¯ä»£ç 500"),
        ("TC009", "è¶…æ—¶å¤„ç†æµ‹è¯•", TestCategory.ERROR_HANDLING, 5000.0, "è¯·æ±‚è¶…æ—¶"),
    ]
    
    for test_id, name, category, duration, error in failed_tests:
        test_results.append(TestResult(
            test_id=test_id,
            name=name,
            description=f"Test case for {name.lower()}",
            expected_result="Test execution should be successful and return expected results",
            test_result="FAIL",
            status=TestStatus.FAIL,
            duration_ms=duration,
            category=category.value,
            error_message=error
        ))
    
    # è·³è¿‡çš„æµ‹è¯•ç”¨ä¾‹
    skipped_tests = [
        ("TC010", "é«˜çº§å®‰å…¨æµ‹è¯•", TestCategory.ERROR_HANDLING, 0),
        ("TC011", "è´Ÿè½½å‹åŠ›æµ‹è¯•", TestCategory.PERFORMANCE, 0),
    ]
    
    for test_id, name, category, duration in skipped_tests:
        test_results.append(TestResult(
            test_id=test_id,
            name=name,
            status=TestStatus.SKIP,
            duration_ms=duration,
            category=category.value
        ))
    
    return test_results


async def test_report_generation():
    """æµ‹è¯•æŠ¥å‘Šç”ŸæˆåŠŸèƒ½"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•KONEæŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ...")
    print("=" * 60)
    
    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    print("ğŸ“Š Step 1: åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
    test_results = create_mock_test_results()
    print(f"   ç”Ÿæˆäº† {len(test_results)} ä¸ªæµ‹è¯•ç»“æœ")
    
    # ç»Ÿè®¡
    passed = sum(1 for t in test_results if t.status == TestStatus.PASS)
    failed = sum(1 for t in test_results if t.status == TestStatus.FAIL)
    skipped = sum(1 for t in test_results if t.status == TestStatus.SKIP)
    
    print(f"   - é€šè¿‡: {passed}")
    print(f"   - å¤±è´¥: {failed}")
    print(f"   - è·³è¿‡: {skipped}")
    
    # 2. å‡†å¤‡å…ƒæ•°æ®
    print("\nğŸ“‹ Step 2: å‡†å¤‡æµ‹è¯•å…ƒæ•°æ®...")
    metadata = {
        "company": "IBC-AI CO.",
        "test_date": datetime.now().isoformat(),
        "api_version": "2.0.0",
        "test_framework": "KONE SR-API v2.0",
        "total_tests": len(test_results),
        "building_id": "DEMO_BUILDING_001",
        "test_environment": "Simulation Mode",
        "tester": "Automation System",
        "version": "2.0.0"
    }
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“„ Step 3: ç”Ÿæˆå¤šæ ¼å¼æŠ¥å‘Š...")
    output_dir = Path("./reports")
    output_dir.mkdir(exist_ok=True)
    
    generator = ReportGenerator("IBC-AI CO.")
    
    reports = generator.generate_report(test_results, metadata, str(output_dir))
    
    # 4. ä¿å­˜æŠ¥å‘Š
    print("\nğŸ’¾ Step 4: ä¿å­˜æŠ¥å‘Šæ–‡ä»¶...")
    
    report_files = {}
    
    for format_type, content in reports.items():
        if format_type == "error":
            print(f"   âŒ æŠ¥å‘Šç”Ÿæˆé”™è¯¯: {content}")
            continue
            
        if format_type == "excel":
            # Excelæ–‡ä»¶å·²ç»ä¿å­˜ï¼Œcontentæ˜¯æ–‡ä»¶è·¯å¾„
            if "not available" in content:
                print(f"   âš ï¸ Excel: {content}")
                continue
            else:
                report_files[format_type] = content
                filepath = Path(content)
                print(f"   âœ… {format_type.upper()}: {filepath} ({filepath.stat().st_size} å­—èŠ‚)")
                continue
                
        # ä¿å­˜å…¶ä»–æ ¼å¼æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"kone_validation_report_{timestamp}.{format_type}"
        filepath = output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        report_files[format_type] = str(filepath)
        print(f"   âœ… {format_type.upper()}: {filepath} ({len(content)} å­—ç¬¦)")
    
    # 5. æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
    print("\nğŸ“– Step 5: æŠ¥å‘Šå†…å®¹é¢„è§ˆ...")
    
    if "markdown" in reports:
        print("\n--- Markdown æŠ¥å‘Šé¢„è§ˆ ---")
        lines = reports["markdown"].split('\n')
        for i, line in enumerate(lines[:20]):  # æ˜¾ç¤ºå‰20è¡Œ
            print(line)
        if len(lines) > 20:
            print("...")
    
    # 6. ç”Ÿæˆæ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“ æŠ¥å‘Šä¿å­˜ä½ç½®: {output_dir.absolute()}")
    print(f"ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
    for format_type, filepath in report_files.items():
        print(f"   - {format_type.upper()}: {Path(filepath).name}")
    
    return report_files


async def test_building_integration():
    """æµ‹è¯•ä¸å»ºç­‘æ•°æ®ç®¡ç†å™¨çš„é›†æˆ"""
    
    print("\nğŸ¢ æµ‹è¯•å»ºç­‘æ•°æ®é›†æˆ...")
    
    try:
        # åˆå§‹åŒ–å»ºç­‘æ•°æ®ç®¡ç†å™¨
        building_manager = BuildingDataManager()
        
        if building_manager.building_config:
            building_id = building_manager.building_config.get('building', {}).get('id', 'Unknown')
            floors = len(building_manager.floor_area_map)
            print(f"   âœ… å»ºç­‘é…ç½®åŠ è½½æˆåŠŸ: {building_id}")
            print(f"   ğŸ—ï¸ æ¥¼å±‚æ•°é‡: {floors}")
            
            # æµ‹è¯•æ•°æ®ç”Ÿæˆ
            test_data = building_manager.generate_test_data("call_elevator", 5)
            print(f"   ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®: {len(test_data)} æ¡è®°å½•")
            
        else:
            print("   âš ï¸ å»ºç­‘é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            
    except Exception as e:
        print(f"   âŒ å»ºç­‘æ•°æ®é›†æˆé”™è¯¯: {e}")


async def test_mapper_integration():
    """æµ‹è¯•ä¸æµ‹è¯•ç”¨ä¾‹æ˜ å°„å™¨çš„é›†æˆ"""
    
    print("\nğŸ—ºï¸ æµ‹è¯•ç”¨ä¾‹æ˜ å°„å™¨é›†æˆ...")
    
    try:
        mapper = TestCaseMapper()
        
        # è·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        all_tests = mapper.get_all_test_cases()
        print(f"   âœ… æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(all_tests)}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for test in all_tests:
            category = test.category.value
            by_category[category] = by_category.get(category, 0) + 1
        
        print("   ğŸ“‹ æŒ‰ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in by_category.items():
            print(f"      - {category}: {count} ä¸ªæµ‹è¯•")
            
        # éªŒè¯ç‰¹å®šæµ‹è¯•ç”¨ä¾‹
        test_case = mapper.get_test_case("initialization_001") 
        if test_case:
            print(f"   ğŸ” ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹: {test_case.name}")
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•æ˜ å°„å™¨é›†æˆé”™è¯¯: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("ğŸ¯ KONE æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ - é›†æˆæµ‹è¯•")
    print("Author: IBC-AI CO.")
    print("=" * 60)
    
    try:
        # æµ‹è¯•å»ºç­‘æ•°æ®é›†æˆ
        await test_building_integration()
        
        # æµ‹è¯•æ˜ å°„å™¨é›†æˆ
        await test_mapper_integration()
        
        # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        report_files = await test_report_generation()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½å¯ç”¨ã€‚")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
